<?xml version="1.0" encoding="ISO-8859-1"?>
<settings>
  <!-- Film parameters -->
  <!-- -1 = start at the beginning, analyze to the end -->
  <start_frame type="int">-1</start_frame>
  <max_frame type="int">-1</max_frame>

  <!-- These are for reducing the analysis AOI -->
  <!--
  <x_start type="int">100</x_start>
  <x_stop type="int">370</x_stop>
  <y_start type="int">0</y_start>
  <y_stop type="int">400</y_stop>
  -->

  <!-- background_sigma -->
  <!-- The sigma of the gaussian to convolve with the image in
       the background estimation step. -->
  <background_sigma type="float">8.0</background_sigma>
  
  <!-- camera calibration file -->
  <!-- This file contains the sCMOS calibration data for the
       region of the camera that the movie comes from. It consists
       of 3 numpy arrays, [offset, variance, gain], each of which
       is the same size as a frame of the movie that is to be
       analyzed. This can be generated for a camera using
       camera_calibration.py and (if it needs to be resliced),
       reslice_calibration.py. -->
  <channel0_cal type="filename">../pgrey_c1.npy</channel0_cal>
  <channel1_cal type="filename">../pgrey_c2.npy</channel1_cal>
  <channel2_cal type="filename">../pgrey_c3.npy</channel2_cal>
  <channel3_cal type="filename">../pgrey_c4.npy</channel3_cal>

  <!-- movie file extensions. -->
  <!-- These are added to the base file name to load the data
       from each of the cameras. -->      
  <channel0_ext type="string">c1.dax</channel0_ext>
  <channel1_ext type="string">c2.dax</channel1_ext>
  <channel2_ext type="string">c3.dax</channel2_ext>
  <channel3_ext type="string">c4.dax</channel3_ext>

  <!-- frame offsets -->
  <!-- This allows you to compensate for the problem that not
       all the cameras start recording on the same frame. -->
  <channel0_offset type="int">0</channel0_offset>
  <channel1_offset type="int">3</channel1_offset>
  <channel2_offset type="int">3</channel2_offset>
  <channel3_offset type="int">3</channel3_offset>

  <!-- The (pixel) radius over which a pixel in the image
       must be a local maximum to be considered as the center
       of a possible localization. -->
  <find_max_radius type="int">2</find_max_radius>

  <!-- Set this to 1 for multi-color data. If it is 0
       then the localizations are forced to have the same
       height in all the channels, which makes sense for
       single color multi-plane data, but not for multi-
       color data where the peak height in each channel
       provides color information. -->
  <independent_heights type="int">1</independent_heights>

  <!-- Maximum number of iterations for new peak finding -->
  <iterations type="int">20</iterations>

  <!-- This file contains how to map localization positions
       in x,y between the different channels. -->
  <mapping type="filename">../map.map</mapping>
  
  <!-- CCD pixel size (in nm) -->
  <pixel_size type="float">120.0</pixel_size>

  <!-- initial guess for sigma -->
  <sigma type="float">1.0</sigma>

  <!-- These are the splines to use for fitting. There should
       be one for each channel. -->
  <spline0 type="filename">../cam1_psf.spline</spline0>
  <spline1 type="filename">../cam2_psf.spline</spline1>
  <spline2 type="filename">../cam3_psf.spline</spline2>
  <spline3 type="filename">../cam4_psf.spline</spline3>
  
  <!-- threshold -->
  <!-- Ideally this is in units of sigma, as in a "x sigma event".
       For example at 3 sigma you'd expect about 0.003 false
       positives per pixel. Incorrect background estimation can
       however complicate things. You probably want to use a
       value greater than 5.0 for most analysis. -->
  <threshold type="float">6.0</threshold>

  <!-- weights -->
  <!-- This file contains information about how to weight
       the per channel localization parameters (i.e. x, y, z
       , etc..) to get the most accurate average value. -->
  <weights type="filename">../weights.npy</weights>

  <!-- z_value. -->
  <!-- Initial z values to consider as starting points for
       localization z locations. -->
  <z_value type="float-array">-500.0,0.0,500.0</z_value>

  <!-- Tracking parameters -->
  <!-- Frame descriptor string
       0 - activation frame
       1 - non-specific frame
       2 - channel1 frame
       3 - channel2 frame
       4 - etc..
       -->
  <descriptor type="string">1</descriptor>

  <!-- Radius for matching peaks from frame to frame -->
  <!-- Localizations that are closer than this value 
       (in pixels) in adjacent frames (ignoring activation
       frames) are assumed to come from the same emitter 
       and are averaged together to create a (hopefully) 
       more accurately localized emitter. 
       If this is zero then no matching will be done. -->
  <radius type="float">0.5</radius>

  <!-- range for z fitting, specified in um. To some
       extent this duplicates the range encoded in
       the spline. You probably want to set these
       values to be larger than those in the spline file. -->
  <min_z type="float">-0.8</min_z>
  <max_z type="float">0.8</max_z>

  <!-- Drift correction parameters -->
  <!-- do drift correction 0 = No -->
  <drift_correction type="int">0</drift_correction>

  <!-- number of frames in each sub-STORM image. -->
  <frame_step type="int">500</frame_step>
  
  <!-- ... 2 is a good value -->
  <!-- This is the "scale" at which to render the sub-STORM
       images for drift correction. Drift correction works 
       by creating STORM images from frame_step sized groups 
       of frames. These are rendered scaled by the d_scale 
       parameter. For example, if your data is 256x256 pixels 
       then the drift-correction will create 512x512 sub-STORM 
       images (for d_scale = 2) and then attempt to correlate 
       these images to each other to calculate the drift. 
       Using a larger d_scale value creates higher resolution 
       sub-STORM images, but they are also sparser so you 
       might not see any improvement in the drift correction. -->
  <d_scale type="int">2</d_scale>  

</settings>


